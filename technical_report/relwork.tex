\section{技术调研}

\subsection{文本分类}

对于文本数据的分类问题，早期方法大多采用支持向量机（SVM）\cite{cortes1995support}、主题模型（LDA）\cite{blei2003latent}等等。随着深度学习方法的兴起，多种神经网络架构也被应用于文本分类问题上。鉴于卷积神经网络（CNN）在视觉图像分类中的良好表现，TextCNN\cite{kim2014convolutional}使用多通道的一维卷积操作提取文本特征，取得了较优的分类效果。相比卷积神经网络仅能提取局部特征的限制，循环神经网络（RNN）\cite{chung2014empirical}在提取全局特征上有着更大的优势，然而循环神经网络无法并行计算，因此其效率往往比较差。基于文档中信息含量不均匀的假设，分层注意力网络（HAN）\cite{yang2016hierarchical}使用注意力机制有效地捕捉文本中的重要信息。为了克服循环神经网络计算效率低的限制，Google基于注意力机制提出了Transformer模型\cite{vaswani2017attention}，利用自注意力机制并行地计算文档表示。同时随着近几年大规模预训练方法的推广，大型自然语言预训练Transformer模型（例如BERT\cite{devlin2019bert}等）在经过微调后可在许多下游任务中取得惊人的表现。由于菜品预测问题所提供的数据集较小，本文仅采用TextCNN架构来构建分类模型。

\subsection{数据增广和正则化}

近年来，许多研究工作提出了多种数据增广方法以解决标注数据不足导致模型泛化性能较差的问题。对于图像数据，一种很自然的想法是将其翻转、裁剪在保持语义不变的情况下生成新的图像，这类方法被广泛应用于视觉模型的训练中\cite{krizhevsky2012imagenet}。而由于文本数据的离散性，数据增广成为一个尚未完全解决的难题。简单数据增广（EDA）\cite{wei2019eda}提供了一种对文本数据进行增广的思路，它采用简单的插入、替换、交换和删除策略，提升了少样本数据上模型的表现。类似地，我们在菜品预测问题中使用数据增广策略，以提升深度神经网络模型的表现。

除了数据增广以外，正则化方法也能通过对模型施加先验约束来提高模型表现。例如广为人知的权重衰减\cite{krogh1992simple}、随机丢弃（Dropout）\cite{srivastava2014dropout}和归一化技术\cite{ba2016layer}都属于正则化方法的范畴之内。标签混合技术\cite{szegedy2016rethinking}通过将独热编码的标签向量与标签空间的均匀分布向量进行混合来避免置信度过高的问题。表征混合方法（MixUp）\cite{zhang2018mixup}以一定的比例随机混合输入特征和标签，使用混合后的表征训练模型，对模型的输入空间施加额外的线性约束以提升泛化性能。浮动方法（Flooding）\cite{ishida2020we}通过避免训练损失下降到零来避免模型过拟合。对抗训练技术\cite{goodfellow2015explaining}不仅能提升模型的鲁棒性，也能通过引入对抗样本对模型进行正则化。参数对抗扰动算法（AMP）\cite{zheng2021regularizing}在每次迭代时向模型参数中注入对抗噪声，以寻找光滑的局部最优解，提高模型的泛化性能。我们选取了权重衰减和表征混合方法作为实验中实际采取的正则化方法，提高了模型在测试集上的表现。

\subsection{集成学习}

在许多学习场景中，我们会面临标注数据匮乏的问题。在训练集较小的情况下，我们往往只能学习到表现比较差的模型，而将多个表现较差的模型进行集成，通常可以得到一个稳定且表现较好的模型，这也就是集成学习\cite{opitz1999popular}的思想。实验证明，将多次训练得到的深度神经网络的输出进行平均后，可以得到一个较稳定的模型\cite{lakshminarayanan2017simple}。在实验中，我们随机训练10个深度神经网络分类模型，取最后一层输出概率的平均值作为每个菜系类别的预测概率，集成后的模型进一步提升了测试集上的准确率。
